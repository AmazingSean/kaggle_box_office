{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from ast import literal_eval\n",
    "import json\n",
    "# import plotly.express as px\n",
    "from ast import literal_eval\n",
    "from imdb import IMDb\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thoughts on variables:\n",
    "- A big **budget** does not necessarily leads to a big revenue, money needs to be spent on the right places.\n",
    "- a **language** might not influence the revenue, but the **number of spoken languages** might do. (check if the number of spoken languages influences the revenue)\n",
    "- **popularity** is a floating varible; it does not seem to influence the revenue so I decided to drop it\n",
    "- **Crew** is definitely an very important factor in predicting the box office. I can import some external data to measure the influence of an individual actor/producer, like the number of followers on Twitter.\n",
    "- Does **ratings** of the movies influence revenues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts about external datasets:\n",
    "- Box offices of movies increasingly rely on their performance in foreign markets; is it possible to integrate the information of foreign market distribution?\n",
    "- Missing budget/revenue data, it can be recovered(scrapped from imdbpro)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "val_set_ratio = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['revenue'] = 0\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "combined = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['imdb_id'] = combined['imdb_id'].astype('str')\n",
    "    \n",
    "#runtime: look for missing runtimes in the imdb dataset; fill the rest with mean\n",
    "ia = IMDb()\n",
    "missing_runtime_df = combined[(combined['runtime'] == 0) | (combined['runtime'].isna())]\n",
    "for movie_id in missing_runtime_df['imdb_id']:\n",
    "    movie = ia.get_movie(movie_id[2:])\n",
    "    imdb_runtime = movie.get('runtimes')\n",
    "    if imdb_runtime is not None:\n",
    "        combined.loc[combined['imdb_id'] == movie_id, 'runtime'] = float(imdb_runtime[0])\n",
    "# fill the rest with mean\n",
    "missing_runtime_df = combined[(combined['runtime'] == 0) | (combined['runtime'].isna())]\n",
    "for movie_id in missing_runtime_df['imdb_id']:\n",
    "    combined.loc[combined['imdb_id'] == movie_id, 'runtime'] = combined['runtime'].mean()\n",
    "\n",
    "# convert crew/cast to list of dicts\n",
    "combined.loc[pd.notna(combined['crew']), 'crew'] = combined[pd.notna(combined['crew'])]['crew'].apply(literal_eval)\n",
    "combined.loc[pd.notna(combined['cast']), 'cast'] = combined[pd.notna(combined['cast'])]['cast'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = combined[combined['is_train'] == 1]\n",
    "test = combined[combined['is_train'] == 0]\n",
    "combined.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['budget', 'runtime', 'revenue']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation between revenue and budget is most obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Budget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['runtime'], y=train['revenue'], height=9, ratio=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date_released:\n",
    "I found date_released to be an really interesting variable. It can be a lens by which we can discover the trends and patterns of the movie market. Before digging into the dataset, I have these questions in mind:\n",
    "- How do revenues fluctuate along these years?\n",
    "- How do revenues fluctuate between different months of each year? \n",
    "- Do lengths of films change?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since only last two digits of year are provided, this is the correct way of getting the year.\n",
    "combined[['release_month','release_day','release_year']]=combined['release_date'].str.split('/',expand=True).replace(np.nan, -1).astype(int)\n",
    "# Some rows have 4 digits of year instead of 2, that's why I am applying (train['release_year'] < 100) this condition\n",
    "combined.loc[ (combined['release_year'] <= 19) & (combined['release_year'] < 100), \"release_year\"] += 2000\n",
    "combined.loc[ (combined['release_year'] > 19)  & (combined['release_year'] < 100), \"release_year\"] += 1900\n",
    "\n",
    "releaseDate = pd.to_datetime(combined['release_date']) \n",
    "combined['release_dayofweek'] = releaseDate.dt.dayofweek\n",
    "combined['release_quarter'] = releaseDate.dt.quarter\n",
    "\n",
    "train = combined[combined['is_train'] == 1]\n",
    "test = combined[combined['is_train'] == 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A General View:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "\n",
    "avg_rev_by_year = train.groupby('release_year')['revenue'].sum()\n",
    "avg_rev_by_year_plot = sns.barplot(x=avg_rev_by_year.index, y=avg_rev_by_year.values)\n",
    "avg_rev_by_year_plot.set_xticklabels(labels=avg_rev_by_year.index, rotation=90)\n",
    "avg_rev_by_year_plot.set(xlabel='Year', ylabel='Average Revenue', title='Average Revenue by Year, 1921-2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that there can be huge difference between average revenues between the years. Interestingly, if you take a look at the history of Hollywood,(TODO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_by_year = train.groupby('release_year')['imdb_id'].count()\n",
    "num_by_year_plot = sns.barplot(x=num_by_year.index, y=num_by_year.values)\n",
    "num_by_year_plot.set_xticklabels(labels=num_by_year.index, rotation=90)\n",
    "num_by_year_plot.set(xlabel='Year', ylabel='Number of Movies Releaed', title='Number of Movies Released by Year, From 1921 To 2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of movies released per year increased along the years obviously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_runtime_by_year = train.groupby('release_year')['runtime'].mean()\n",
    "avg_runtime_by_year_plot = sns.barplot(x=avg_runtime_by_year.index, y =avg_runtime_by_year.values)\n",
    "avg_runtime_by_year_plot.set_xticklabels(labels=avg_runtime_by_year.index, rotation=90)\n",
    "avg_runtime_by_year_plot.set(xlabel='Year', ylabel='Average Runtime', title='Average Runtime of Movies, 1921-2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There no clear trend for the length of movies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revenue by Month / Number of Movies Released by Month: (Release_Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rev_by_month = train.groupby('release_month')['revenue'].mean()\n",
    "avg_rev_by_month_plot = sns.barplot(x= avg_rev_by_month.index, y=avg_rev_by_month.values)\n",
    "avg_rev_by_month_plot.set(xlabel='Month', ylabel='Average Revenue', title='Average Revenues by Month, 1921-2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By observing the average monthly revenues through 1921 to 2017, a trend can be found that average revenues were lowest between Jan to Feb and again Aug to Sept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[train['release_year'] > 2010]\n",
    "avg_rev_by_year_and_month = df.groupby(['release_year', 'release_month'], as_index=False)['revenue'].mean()\n",
    "avg_rev_by_year_and_month_plot = sns.lineplot(data=avg_rev_by_year_and_month, x='release_month', y='revenue', hue = 'release_year', legend = 'full', palette='Set1')\n",
    "avg_rev_by_year_and_month_plot.set(xlabel='Month', ylabel='Revenue', title='Monthly Average Revenues by Month, 2011-2017')\n",
    "avg_rev_by_year_and_month_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The same trend can be found when taking a closer look at monthly average revenues between 2010-2017.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_movies_by_year_and_month = df.groupby(['release_year', 'release_month'], as_index=False)['imdb_id'].count()\n",
    "num_of_movies_by_year_and_month_plot = sns.lineplot(data=num_of_movies_by_year_and_month, x='release_month', y='imdb_id', hue='release_year', legend='full', palette='Set1')\n",
    "num_of_movies_by_year_and_month_plot.set(xlabel='Month', ylabel='Number of Movies released', title='Monthly Number of Released Movies, 2011-2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By contrast, August to October see the most number of movies released when we look at the data from 2011-2017. (The same pattern persists for previous years also.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**A closer look at this trend is provided below, where we can compare the average revenues with number of releases each year:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_monthly_avg_rev_by_year(year):\n",
    "    df = train[train['release_year'] == year]\n",
    "    table = df.groupby('release_month')['revenue'].mean()\n",
    "    avg_rev_by_month_plot = sns.barplot(x=table.index, y=table.values)\n",
    "    avg_rev_by_month_plot.set(xlabel='Month', ylabel='Average Revenue', title='Average Revenues by Month in '+ str(year))\n",
    "    plt.show()\n",
    "    \n",
    "def display_monthly_num_of_movies_by_year(year):\n",
    "    df = train[train['release_year'] == year]\n",
    "    table = df.groupby('release_month', as_index=False)['imdb_id'].count()\n",
    "    plot = sns.barplot(data=table, x='release_month', y='imdb_id')\n",
    "    plot.set(xlabel='Month', ylabel='Number of Movies released', title='Number of Released Movies by Month in ' + str(year))\n",
    "    plt.show()\n",
    "\n",
    "def compare_num_with_rev_by_year(year):\n",
    "    display_monthly_avg_rev_by_year(year)\n",
    "    display_monthly_num_of_movies_by_year(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_num_with_rev_by_year(2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trend can be identified by comparing monthly number of movies released and monthly average revenues: they seem to be inversely correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(s):\n",
    "    try:\n",
    "        d = eval(s)\n",
    "    except:\n",
    "        d = {}\n",
    "    return d\n",
    "\n",
    "combined['genres'] = combined['genres'].map(lambda x: sorted([d['name'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str, x)))\n",
    "genres = combined.genres.str.get_dummies(sep=',')\n",
    "combined = pd.concat([combined, genres], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = combined[combined['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test = combined[combined['is_train'] == 0].drop(['is_train', 'revenue'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the revenues between different genres throughout the years:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total revenue and avg revenue for each genre in each year, in order to plot the animation plot:\n",
    "# colnames: 'genre', 'year', 'avg_rev', 'total_rev', 'num_of_productions'\n",
    "def get_genre_animation_df(dataset, years, genres):\n",
    "    result =[]\n",
    "    for year in years:\n",
    "        df_by_year = dataset[dataset['release_year'] == year]\n",
    "        for genre in genres:\n",
    "            df_by_year_and_genre = df_by_year[df_by_year[genre] == 1]\n",
    "            result.append([year, genre, df_by_year_and_genre['revenue'].mean(), \n",
    "                           df_by_year_and_genre['revenue'].sum(),df_by_year_and_genre.shape[0]])\n",
    "            \n",
    "    return result\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_genre_animation_df(train, np.arange(1912,2018), genres.columns)\n",
    "genre_animation_plot_df = pd.DataFrame(df, columns=['year', 'genre', 'avg_rev', 'total_rev', 'num_of_productions'])\n",
    "genre_animation_plot_df = genre_animation_plot_df.fillna(0)\n",
    "px.scatter(genre_animation_plot_df, x='avg_rev', y = 'total_rev', animation_frame = 'year',\n",
    "          size = 'num_of_productions', color='genre', hover_name = 'genre',\n",
    "          size_max=55, range_x = [-3e+8, 7e+08], range_y = [-3e+8, 9e+09])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we count movies multiple times if it has multiple genres\n",
    "def get_unpacked_df_by_genre(dataset, genres):\n",
    "    result = pd.DataFrame(columns=['title', 'genre', 'revenue','budget'])\n",
    "    for genre in genres:\n",
    "        genre_df = dataset[dataset[genre] == 1][['title', 'genres', 'revenue', 'budget']]\n",
    "        genre_df['genres'] = genre\n",
    "        genre_df.columns = ['title', 'genre', 'revenue', 'budget']\n",
    "        result = pd.concat([result, genre_df], ignore_index=True)\n",
    "    return result\n",
    "\n",
    "df_by_genre = get_unpacked_df_by_genre(train, genres.columns)\n",
    "px.box(df_by_genre, x= 'genre', y='revenue')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As it can be seen from both the animation plot and boxplot, genres like Advenure/Animation/Action/Fantasy are likely to make more revenues, as expected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring language related variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "[train['original_language'].count(), train['spoken_languages'].count()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NA count\n",
    "(train['spoken_languages'].isna()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NaN columns of spoken_languages variable\n",
    "\n",
    "#train['spoken_languages'].index('')\n",
    "#print(train[train[\"spoken_languages\"].isnull()][null_columns])\n",
    "#NaN in train['spoken_languages']\n",
    "train[train['spoken_languages'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['original_language'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spoken_languages'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engineering spoken_languages column\n",
    "\n",
    "#train['spoken_languages'].map(lambda x: sorted([d['iso_639_1'] for d in get_dictionary(x)])).map(lambda x: ','.join(map(str,x)))\n",
    "train['spoken_languages_two'] = train['spoken_languages'].map(lambda x: sorted([d['iso_639_1'] for d in get_dictionary(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spoken_languages_two'].head(15)\n",
    "train['spoken_languages_two'][3]\n",
    "train['spoken_languages_two'][3][1]\n",
    "train['spoken_languages_two'][150] #empty for original NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spoken_languages_two'].isna().sum() #0\n",
    "train['spoken_languages_two'].count() #3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['original_language'][1] in train['spoken_languages_two'][1]\n",
    "train['orig_in_spok_language'] = np.array(0)\n",
    "\n",
    "#for i in train['original_language']:\n",
    " #   if (train['original_language'][i] in train['spoken_languages_two'][i] == TRUE):\n",
    "  #      train['orig_in_spok_language'] = 1\n",
    "   # else:\n",
    "    #    train['orig_in_spok_language'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = []\n",
    "for idx, row in train.iterrows():\n",
    "    index_list.append(row['original_language'] in row['spoken_languages_two'])\n",
    "\n",
    "index_list #True means the original language is contained in spoken languages\n",
    "len(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_index_list = [not i for i in index_list]\n",
    "not_index_list #reverse of index_list\n",
    "\n",
    "train[not_index_list] #those whose original language is not a part of spoken languages\n",
    "len(train[not_index_list].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index_list.index(False) #92: first row of the one that is false\n",
    "sum(not_index_list)\n",
    "index_list.count(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of movies according to original languages usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['original_language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train['original_language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20,15))\n",
    "#sns.countplot(train['original_language'].sort_values())#alphabetical order\n",
    "#plt.ylabel('Number of Movies')\n",
    "#plt.xlabel('Original Languages')\n",
    "#plt.title(\"Counts for Original Languages of Movies (alphabetical order)\",fontsize=20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby(['original_language']).original_language.count().nlargest(36) #top 5 of all 36 languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(20,15))\n",
    "#train_OrigLang=train.groupby(['original_language']).original_language.count().nlargest(36)\n",
    "#bar1=sns.barplot(train_OrigLang.index, train_OrigLang.values,alpha=0.8)\n",
    "\n",
    "#plt.ylabel('Number of Movies')\n",
    "#plt.xlabel('Original Languages')\n",
    "#plt.title(\"Counts for Original Languages of Movies (descending order)\",fontsize=20)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pos = train['original_language']\n",
    "#performance = train['revenue']\n",
    "\n",
    "#plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "#plt.ylabel('Revenue')\n",
    "#plt.title('Revenue of Movies with Different Original Languages')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RevByOrigLang=train.groupby(['original_language']).revenue.mean()\n",
    "train_RevByOrigLang.nlargest(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RevByOrigLang=train.groupby(['original_language']).revenue.mean()\n",
    "train_RevByOrigLang=train_RevByOrigLang.nlargest(36) #descending order, 36 original languages in total\n",
    "\n",
    "#plt.figure(figsize=(13,5))\n",
    "bar2=sns.barplot(train_RevByOrigLang.index, train_RevByOrigLang.values,alpha=0.8)\n",
    "#bar2.set_ylim([0,100000000])\n",
    "plt.ylabel(\"Mean Revenue\")\n",
    "plt.title(\"Mean Revenue of Movies with Different Original Languages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast/Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 1. scrape data online to fill in the nan\n",
    "crew_cast_dataset = train[pd.notnull(train['cast']) & pd.notnull(train['crew'])]\n",
    "crew_cast_dataset['cast'] = crew_cast_dataset['cast'].apply(literal_eval)\n",
    "crew_cast_dataset['crew'] = crew_cast_dataset['crew'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct two features:\n",
    "# economic_idx: total revenues of the top five cast/crew members, ranked by the average revenue of their most recent five films\n",
    "# artistic_idx: the number of crew/cast members who are involved in the production of a film nominated for Academy Award\n",
    "def was_involved(dict_list, name):\n",
    "    \"\"\"\n",
    "    Check if the name got involved in the production of this film.\n",
    "    \"\"\"\n",
    "    was_involved = False\n",
    "    for name_dict in dict_list:\n",
    "        if name_dict['name'] == name:\n",
    "            was_involved = True\n",
    "    return was_involved\n",
    "    \n",
    "    \n",
    "def get_recent_five_avg_rev(name, cast_or_crew, year, month):\n",
    "    \"\"\"\n",
    "    Get a list of the revenues of 5 most recent films by name.\n",
    "    The list length can be shorter than 5.\n",
    "    \"\"\"\n",
    "    # filter the dataset by time to find the most recent five films\n",
    "    target_df = crew_cast_dataset[crew_cast_dataset['release_year'] <= year]\n",
    "    # drop the rows with year equal to target year but month larger than target month\n",
    "    drop_idx = target_df[(target_df['release_year'] == year) & (target_df['release_month'] >= month)].index\n",
    "    target_df = target_df.drop(drop_idx)\n",
    "    \n",
    "    \n",
    "    # subset the target_df to get the rows that contain the cast/crew\n",
    "    if cast_or_crew == 'cast':\n",
    "        target_df = target_df[target_df.apply(lambda row: was_involved(row['cast'], name), axis=1)]\n",
    "    \n",
    "    if cast_or_crew == 'crew':\n",
    "        target_df = target_df[target_df.apply(lambda row: was_involved(row['crew'], name), axis=1)]\n",
    "    \n",
    "    # sort the dataframe by release date:\n",
    "    target_df.sort_values(by=['release_year', 'release_month', 'release_day'], ascending = False)\n",
    "    \n",
    "    avg_rev = target_df['revenue'][:5].mean()\n",
    "    \n",
    "    if math.isnan(avg_rev):\n",
    "        return 0\n",
    "    else:\n",
    "        return avg_rev\n",
    "    \n",
    "    \n",
    "\n",
    "def get_economic_idx(row):\n",
    "    cast_rev_list = []\n",
    "    crew_rev_list = []\n",
    "    for cast_dict in row['cast']:\n",
    "        print(\"=============================\\nName: \" + str(cast_dict['name']) + str(row['release_year']) + str(row['release_month']))\n",
    "        rev = get_recent_five_avg_rev(cast_dict['name'], 'cast', row['release_year'], row['release_month'])\n",
    "        print(\"Revenue: \" + str(rev))\n",
    "        cast_rev_list.append(rev)              \n",
    "    for crew_dict in row['crew']:\n",
    "        crew_rev_list.append(get_recent_five_avg_rev(crew_dict['name'], 'crew', row['release_year'], row['release_month']))\n",
    "        \n",
    "    return (sum(sorted(cast_rev_list, reverse=False)[:5]) + sum(sorted(crew_rev_list, reverse=False)[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get economic_idx, the computation is really expensive so we save the dataset and reuse the results here:\n",
    "# ['economic_idx'] = crew_cast_dataset.apply(lambda row: get_economic_idx(row), axis=1)\n",
    "# crew_cast_dataset.to_csv('../data/crew_cast_dataset.csv')\n",
    "crew_cast_dataset = pd.read_csv('../data/crew_cast_dataset.csv')\n",
    "crew_cast_dataset['cast'] = crew_cast_dataset['cast'].apply(literal_eval)\n",
    "crew_cast_dataset['crew'] = crew_cast_dataset['crew'].apply(literal_eval)\n",
    "crew_cast_dataset['economic_idx'].corr(crew_cast_dataset['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct artistic_idx:\n",
    "oscar_nominations = pd.read_csv('../data/oscar_nominations.csv')\n",
    "oscar_nominations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oscar_nominee_count(dict_list):\n",
    "    nominee_count = 0\n",
    "    for name_dict in dict_list:\n",
    "        print(name_dict['name'])\n",
    "        if name_dict['name'] in oscar_nominations['Name']:\n",
    "            nominee_count = nominee_count + 1\n",
    "    return nominee_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['was_nominated_for_oscar'] = train['title'].isin(oscar_nominations.Film)\n",
    "px.box(train, x='was_nominated_for_oscar', y='revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataset the get the films that were nominated for oscar\n",
    "oscar_films = train[train['was_nominated_for_oscar'] == True]\n",
    "\n",
    "\n",
    "# def get_num_of_nominated_films(target_df, name):\n",
    "#     \"\"\"\n",
    "#     Get the number of times a name was involved in the production of an oscar-nominated film\n",
    "#     \"\"\"\n",
    "#     for idx, row in target_df\n",
    "    \n",
    "    \n",
    "#     count = sum(target_df['crew'].apply(lambda x: was_involved(name, )))\n",
    "\n",
    "def get_artistic_idx(row):\n",
    "    count = 0\n",
    "    # filter the dataset to be earlier than the year and month:\n",
    "    target_df = oscar_films[oscar_films['release_year'] <= row['release_year']]\n",
    "    # print(target_df.index)\n",
    "    drop_idx = oscar_films[(oscar_films['release_year'] == row['release_year']) & (oscar_films['release_month'] >= row['release_month'])].index\n",
    "    # print(oscar_films[(oscar_films['release_year'] == row['release_year']) & (oscar_films['release_month'] >= row['release_month'])])\n",
    "    target_df = target_df.drop(drop_idx)\n",
    "    \n",
    "    for name_dict in row['cast']:\n",
    "        target_name = name_dict['name']\n",
    "        target_name_count = 0\n",
    "        for idx, row in target_df.iterrows():\n",
    "            # count the number of times this name appears in the cast column:\n",
    "            for oscar_name_dict in row['cast']:\n",
    "                if target_name == oscar_name_dict['name']:\n",
    "                    target_name_count = target_name_count + 1\n",
    "            # count the number of times this name appears in the crew column:\n",
    "            for oscar_name_dict in row['crew']:\n",
    "                if target_name == oscar_name_dict['name']:\n",
    "                    target_name_count = target_name_count + 1\n",
    "        count = count + target_name_count\n",
    "        \n",
    "    for name_dict in row['crew']:\n",
    "        target_name = name_dict['name']\n",
    "        target_name_count = 0\n",
    "        for idx, row in target_df.iterrows():\n",
    "            # count the number of times this name appears in the cast column:\n",
    "            for oscar_name_dict in row['cast']:\n",
    "                if target_name == oscar_name_dict['name']:\n",
    "                    target_name_count = target_name_count + 1\n",
    "            # count the number of times this name appears in the crew column:\n",
    "            for oscar_name_dict in row['crew']:\n",
    "                if target_name == oscar_name_dict['name']:\n",
    "                    target_name_count = target_name_count + 1\n",
    "        count = count + target_name_count\n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[pd.notnull(train['cast']) & pd.notnull(train['crew'])]\n",
    "# train['cast'] = train['cast'].apply(literal_eval)\n",
    "# train['crew'] = train['crew'].apply(literal_eval)\n",
    "# train['artistic_idx'] = train.apply(lambda row: get_artistic_idx(row), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(train, x='artistic_idx', y = 'revenue')\n",
    "fig.update_layout(yaxis_type=\"log\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It can be seen that movies nominated for Oscar tend to generate more revenues.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(train_df):\n",
    "    # TODO: add collection_idx\n",
    "    # TODO: how to deal with budget?\n",
    "    # TODO: add production_company_idx\n",
    "    # TODO: spoken_language?\n",
    "    # TODO: Keywords?\n",
    "    \n",
    "    # original_language:\n",
    "    original_language_dummies = pd.get_dummies(train_df['original_language'])\n",
    "    train_df = pd.concat([train_df, original_language_dummies], axis=1, sort=False)\n",
    "    \n",
    "    train_df['num_of_production_companies'] = train_df['production_companies'].apply(lambda x: len(literal_eval(x)) if pd.notnull(x) else x)\n",
    "    train_df['num_of_production_countries'] = train_df['production_countries'].apply(lambda x: len(literal_eval(x)) if pd.notnull(x) else x)\n",
    "    train_df['num_of_spoken_languages'] = train_df['spoken_languages'].apply(lambda x: len(literal_eval(x)) if pd.notnull(x) else x)\n",
    "    \n",
    "    \n",
    "    # TODO: take log of the revenue\n",
    "    \n",
    "    train_df = train_df.drop(['id', 'status', 'homepage', 'original_title', 'overview', 'poster_path', 'tagline', 'cast', 'crew', \n",
    "                              'belongs_to_collection', 'genres', 'imdb_id', 'original_language', \n",
    "                              'production_companies', \n",
    "                             'production_countries', 'release_date', 'spoken_languages', 'title', 'Keywords'], axis=1)\n",
    "    \n",
    "    return train_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = engineer_features(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = combined[combined['is_train'] == 1].drop(['is_train'], axis=1)\n",
    "test = combined[combined['is_train'] == 0].drop(['is_train', 'revenue'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['revenue'] = np.log1p(train['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['revenue'].values\n",
    "train = train.drop(columns=['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# import lightgbm as lgb\n",
    "# from catboost import CatBoostRegressor\n",
    "\n",
    "random_seed = 2019\n",
    "k = 10\n",
    "fold = list(KFold(k, shuffle = True, random_state = random_seed).split(train))\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def xgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "    \n",
    "    params = {'objective': 'reg:linear', \n",
    "              'eta': 0.01, \n",
    "              'max_depth': 6, \n",
    "              'subsample': 0.6, \n",
    "              'colsample_bytree': 0.7,  \n",
    "              'eval_metric': 'rmse', \n",
    "              'seed': random_seed, \n",
    "              'silent': True,\n",
    "    }\n",
    "    \n",
    "    record = dict()\n",
    "    model = xgb.train(params\n",
    "                      , xgb.DMatrix(trn_x, trn_y)\n",
    "                      , 100000\n",
    "                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n",
    "                      , verbose_eval=verbose\n",
    "                      , early_stopping_rounds=500\n",
    "                      , callbacks = [xgb.callback.record_evaluation(record)])\n",
    "    best_idx = np.argmin(np.array(record['valid']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n",
    "    test_pred = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    return {'val':val_pred, 'test':test_pred, 'error':record['valid']['rmse'][best_idx], 'importance':[i for k, i in model.get_score().items()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def lgb_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "\n",
    "    params = {'objective':'regression',\n",
    "         'num_leaves' : 30,\n",
    "         'min_data_in_leaf' : 20,\n",
    "         'max_depth' : 9,\n",
    "         'learning_rate': 0.004,\n",
    "         #'min_child_samples':100,\n",
    "         'feature_fraction':0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9,\n",
    "         'lambda_l1': 0.2,\n",
    "         \"bagging_seed\": random_seed,\n",
    "         \"metric\": 'rmse',\n",
    "         #'subsample':.8, \n",
    "          #'colsample_bytree':.9,\n",
    "         \"random_state\" : random_seed,\n",
    "         \"verbosity\": -1}\n",
    "\n",
    "    record = dict()\n",
    "    model = lgb.train(params\n",
    "                      , lgb.Dataset(trn_x, trn_y)\n",
    "                      , num_boost_round = 100000\n",
    "                      , valid_sets = [lgb.Dataset(val_x, val_y)]\n",
    "                      , verbose_eval = verbose\n",
    "                      , early_stopping_rounds = 500\n",
    "                      , callbacks = [lgb.record_evaluation(record)]\n",
    "                     )\n",
    "    best_idx = np.argmin(np.array(record['valid_0']['rmse']))\n",
    "\n",
    "    val_pred = model.predict(val_x, num_iteration = model.best_iteration)\n",
    "    test_pred = model.predict(test, num_iteration = model.best_iteration)\n",
    "    \n",
    "    return {'val':val_pred, 'test':test_pred, 'error':record['valid_0']['rmse'][best_idx], 'importance':model.feature_importance('gain')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "def cat_model(trn_x, trn_y, val_x, val_y, test, verbose) :\n",
    "    \n",
    "    model = CatBoostRegressor(iterations=100000,\n",
    "                                 learning_rate=0.004,\n",
    "                                 depth=5,\n",
    "                                 eval_metric='RMSE',\n",
    "                                 colsample_bylevel=0.8,\n",
    "                                 random_seed = random_seed,\n",
    "                                 bagging_temperature = 0.2,\n",
    "                                 metric_period = None,\n",
    "                                 early_stopping_rounds=200\n",
    "                                )\n",
    "    model.fit(trn_x, trn_y,\n",
    "                 eval_set=(val_x, val_y),\n",
    "                 use_best_model=True,\n",
    "                 verbose=False)\n",
    "    \n",
    "    val_pred = model.predict(val_x)\n",
    "    test_pred = model.predict(test)\n",
    "    \n",
    "    return {'val':val_pred, \n",
    "            'test':test_pred, \n",
    "            'error':model.get_best_score()['validation']['RMSE']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict()\n",
    "val_pred = np.zeros(train.shape[0])\n",
    "test_pred = np.zeros(test.shape[0])\n",
    "final_err = 0\n",
    "verbose = False\n",
    "\n",
    "for i, (trn, val) in enumerate(fold) :\n",
    "    print(i+1, \"fold.    RMSE\")\n",
    "    \n",
    "    trn_x = train.loc[trn, :]\n",
    "    trn_y = y[trn]\n",
    "    val_x = train.loc[val, :]\n",
    "    val_y = y[val]\n",
    "    \n",
    "    fold_val_pred = []\n",
    "    fold_test_pred = []\n",
    "    fold_err = []\n",
    "    \n",
    "    #\"\"\" xgboost\n",
    "    start = datetime.now()\n",
    "    result = xgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val']*0.2)\n",
    "    fold_test_pred.append(result['test']*0.2)\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    #\"\"\"\n",
    "    \n",
    "    #\"\"\" lightgbm\n",
    "    start = datetime.now()\n",
    "    result = lgb_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val']*0.4)\n",
    "    fold_test_pred.append(result['test']*0.4)\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"lgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    #\"\"\"\n",
    "    \n",
    "    #\"\"\" catboost model\n",
    "    start = datetime.now()\n",
    "    result = cat_model(trn_x, trn_y, val_x, val_y, test, verbose)\n",
    "    fold_val_pred.append(result['val']*0.4)\n",
    "    fold_test_pred.append(result['test']*0.4)\n",
    "    fold_err.append(result['error'])\n",
    "    print(\"cat model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds/60)) + 'm)')\n",
    "    #\"\"\"\n",
    "    \n",
    "    # mix result of multiple models\n",
    "    val_pred[val] += np.mean(np.array(fold_val_pred), axis = 0)\n",
    "    #print(fold_test_pred)\n",
    "    #print(fold_test_pred.shape)\n",
    "    #print(fold_test_pred.columns)\n",
    "    test_pred += np.mean(np.array(fold_test_pred), axis = 0) / k\n",
    "    final_err += (sum(fold_err) / len(fold_err)) / k\n",
    "    \n",
    "    print(\"---------------------------\")\n",
    "    print(\"avg   err.\", \"{0:.5f}\".format(sum(fold_err) / len(fold_err)))\n",
    "    print(\"blend err.\", \"{0:.5f}\".format(np.sqrt(np.mean((np.mean(np.array(fold_val_pred), axis = 0) - val_y)**2))))\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "print(\"fianl avg   err.\", final_err)\n",
    "print(\"fianl blend err.\", np.sqrt(np.mean((val_pred - y)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../data/sample_submission.csv')\n",
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = sub['id']\n",
    "df_sub['revenue'] = np.expm1(test_pred*3)\n",
    "#print(df_sub['revenue'])\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary for release_year and release_month:\n",
    "1. Revenues are generally low during Jan to Feb, also Aug to Sept. There are called Dump Months, more information can be found at:\n",
    "    - https://www.wikiwand.com/en/Dump_months\n",
    "    - https://web.archive.org/web/20151012234912/http://www.theatlantic.com/entertainment/archive/2012/01/january-dumping-ground-for-terrible-movies-like-contraband/251326/\n",
    "    - https://web.archive.org/web/20150922062108/http://www.avclub.com/article/hollywoods-trash-our-treasure-17-salvageable-flops-91570\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['spoken_languages'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = {'Country': 'China', 'Capital': 'Beijing'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_kernel",
   "language": "python",
   "name": "py36"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "450px",
    "width": "476px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
